---
title: "HW_04"
author: "RChandra"
date: "2025-12-03"
output: html_document
---
I created a virtual environment using the `virtualenv_create("HW_04venv")` command
## R Setup Chunk:
```{r setup, include=FALSE}
  library(reticulate)
  use_virtualenv("HW_04venv", required = TRUE)
  knitr::opts_chunk$set(echo = TRUE)
```
## Load Libraries
I saved the the ACS microdata file `usa_00005.dta` that Gabe shared on Populi in my ML_better folder on my desktop. Core libraries for this assignment will be: pandas, numpy, and os.Python code begins:
```{python}
   import pandas as pd
   import numpy as np
   import os
   print("Libraries loaded successfully!")
```
Setting the virtual environment as my current working directory:
```{python}
  os.chdir("C:/Users/rchandra/Desktop/Schoolwork/Fall 2025/Machine Learning and Data Science/ML_better/Homework")
```
Verify the change:
```{python}
  print("New working directory:", os.getcwd())
```
Load and check data:
```{python}
 df = pd.read_stata("Raw Data/usa_00005.dta")
  print("Data loaded successfully!")
  print("Shape of dataset:", df.shape)
```
## Task 1: Load and Log‑transform ACS 
Step 1: Drop rows of data with invalid incomes
```{python}
  df = df[df["inctot"] != 9999999]
```
Step 2: Sample 10% of the data for faster processing
```{python}
  df_sample = df.sample(frac=0.10, random_state=42)
    print("Sampled data shape:", df_sample.shape)
    print("Income column summary:")
    print(df_sample["inctot"].describe())
```
### Question 1: Why is it not necessary to impute or filter our missing values for other features?

**My Answer:** Since we are only performing exploratory modeling and sampling a large dataset, missing values are rare or encoded as specific numeric codes like we discussed in class (like 9999999). That's why we only need to remove invalid income values because they directly affect the target variable.

### Task 1.1: Log-transform Income.

Instead of predicting income directly, we will predict log-income. Create a new target column `log_inctot` by applying the natural log transformation to `inctot` after adding 1 (i.e., `np.log1p`). Filter out any rows where `inctot` is less than or equal to zero before applying the log transformation. Make a histogram of both the original income and log-income distributions to visualize the effect of the transformation.

**My Code:**

```{python}
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
```

Filtering out nonpositive income values:

```{python}
df_sample = df_sample[df_sample["inctot"] > 0]
```

Creating the log-transformed income column

```{python}
df_sample["log_inctot"] = np.log1p(df_sample["inctot"])
print("Log transformation complete!")
print(df_sample[["inctot", "log_inctot"]].head())
```

Creating the Histograms:

```{python}
  plt.figure(figsize=(12,5))
    plt.subplot(1,2,1)
    sns.histplot(df_sample["inctot"], bins=50, kde=True)
  plt.title("Original Income Distribution")
  plt.xlabel("Income")
    plt.subplot(1,2,2)
    sns.histplot(df_sample["log_inctot"], bins=50, kde=True)
  plt.title("Log-Transformed Income Distribution")
  plt.xlabel("Log(Income + 1)")
    plt.tight_layout()
    plt.show()
```
### Question 2: What are the advantages of predicting log-income rather than income directly?

**My Answer:** The raw distribution is heavily right skewed, which reflects reality but is harder for neural networks to understand and process, so the logarithmic transformation allows the distribution to look more like a normal distribution and therefore makes it easier for the data to be process-able.

## Task 2: Feature Engineering and Encoding
Split the data into a feature matrix `X` and target vector `y` (the log-income column created above). Perform one-hot encoding on all categorical features, dropping sparse columns whose proportion of non-zero entries falls below 1%. The feature matrix `X` should include all columns except `inctot` and `log_inctot`. The target vector `y` should be the `log_inctot` column. Mimic the one-hot encoding workflow from the tutorial: build dummies for every categorical column in `X`, then drop sparse columns whose proportion of non-zero entries falls below 1%. 
Step One: label the target vector and feature matrix as such with the following code. 
**My Code:** 
```{python}
y = df_sample["log_inctot"]
X = df_sample.drop(columns=["inctot", "log_inctot"])
print("Feature matrix shape:", X.shape)
print("Target vector shape:", y.shape)
```
Step Two: Have the program identify the categorical columns (object or categorical dtype):
```{python}
  cat_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()
    print("Categorical columns:", cat_cols)
```
Step Two: Have the program identify the categorical columns (object or categorical dtype):

```{python}
  cat_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()
    print("Categorical columns:", cat_cols)
```

Step Three: Mimic the one-hot encoding workflow we learned in class.

```{python}
  X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=False)
   print("Encoded feature matrix shape:", X_encoded.shape)
```

Step Four: Compute the proportion of non‑zero entries for each column.

```{python}
  nonzero_prop = (X_encoded != 0).sum(axis=0) / len(X_encoded)
```

Step Five: Drop sparse columns (those less than 1% non‑zero entries).

```{python}
  X_filtered = X_encoded.loc[:, nonzero_prop >= 0.01]
    print("Filtered feature matrix shape:", X_filtered.shape)
```
### Task 2.1: Train/Test Split
Split the encoded data into training and testing sets (80/20).

```{python}
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X_filtered, y, test_size=0.2, random_state=42
)
      print("Training set shape:", X_train.shape)
      print("Testing set shape:", X_test.shape)
```

## Task 3: XGBoost Modeling (Log Target)
Reproduce the final modeling attempt from `examples/XGBoost.ipynb`, but treat the log-income target as the label. Work through the following subtasks and insert your own code in each block.

### Task 3.1: Build DMatrix Objects
Create `xgb.DMatrix` objects for both the training and test splits using the log-income labels.
Prepare data for XGBoost by converting the pandas DataFrames into numeric arrays (NumPy arrays)

```{python}
  dtrain = xgb.DMatrix(X_train, label=y_train)
  dtest = xgb.DMatrix(X_test, label=y_test)
    print("DMatrix objects created successfully!")
```

### Task 3.2: Hyperparameter Dictionary
Start from the tutorial’s baseline settings (e.g., `eta = 1`, `max_depth = 10`, `min_child_weight = 100`, RMSE metric, early stopping after 50 rounds) and justify any deviations you make.
Use the `reg:squarederror` objective and `rmse` evaluation metric.

```{python}
  params = {
    "objective": "reg:squarederror",  # regression task
    "eval_metric": "rmse",            # root mean squared error
    "eta": 1,                         # learning rate (baseline)
    "max_depth": 10,                  # maximum tree depth
    "min_child_weight": 100,          # minimum sum of instance weight (hessian) needed in a child
    "subsample": 0.8,                 # fraction of rows sampled per tree
    "colsample_bytree": 0.8,          # fraction of features sampled per tree
    "seed": 42                        # reproducibility
}

print("Hyperparameter dictionary defined:")
for k, v in params.items():
    print(f"  {k}: {v}")
```

**My Deviations:** I’ve decided to deviate from the previous baseline parameters to better reflect the complexity of real‑world income data. I will reduce the learning rate (eta) from 1 to 0.1 so the model learns more gradually, which is important because income relationships like the effect of education or occupation tend to be cumulative rather than sudden. I decreased the maximum tree depth (max_depth) from 10 to 6 to prevent the model from memorizing overly specific combinations of demographic traits (for example, “40‑year‑old engineers in California with master’s degrees”), focusing instead on broader, generalizable patterns like education level and industry. I also lowered the minimum child weight (min_child_weight) from 100 to 10 to allow the model to capture smaller, more extreme subgroups—such as individuals working unusually long hours or those with very high education but low reported income—since these edge cases can be meaningful in understanding income inequality. I kept the subsample and column sampling rates (subsample and colsample_bytree) at 0.8 to maintain randomness and avoid overfitting. Overall, these adjustments make the model more sensitive to realistic variations in socioeconomic data while still maintaining stability and generalization.

```{python}
params = {
    "objective": "reg:squarederror",  # regression task
    "eval_metric": "rmse",            # root mean squared error
    "eta": 0.1,                       # lower learning rate for smoother convergence
    "max_depth": 6,                   # shallower trees to reduce overfitting
    "min_child_weight": 10,           # allows smaller leaves to capture finer patterns
    "subsample": 0.8,                 # row sampling for regularization
    "colsample_bytree": 0.8,          # feature sampling for regularization
    "seed": 42                        # reproducibility
}

early_stopping_rounds = 50  # stop if no improvement for 50 rounds
    print("Adjusted hyperparameter dictionary:")
for k, v in params.items():
    print(f"  {k}: {v}")
    print(f"early_stopping_rounds: {early_stopping_rounds}")
```

### Task 3.3: Hyperparameter Tuning

Perform a grid search over the `eta = [1, 0.1, 0.01]` and `max_depth = [3, 6, 10]` parameters to identify the best combination based on validation RMSE (i.e., for each combination of `eta` and `max_depth`, train a model and record the validation RMSE). Use early stopping with a patience of 50 rounds and a maximum of 10,000 training rounds. This is going to be a total of 3X3=9 total models

**The Training Loop Code Chunk:** Storing the Results:

```{python}
import xgboost as xgb
import pandas as pd

  etas = [1, 0.1, 0.01]
  depths = [3, 6, 10]
  
  base_params = {
    "objective": "reg:squarederror",
    "eval_metric": "rmse",
    "min_child_weight": 10,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "seed": 42
}

  early_stopping_rounds = 50
  num_rounds = 10000
  
  dtrain = xgb.DMatrix(X_train, label=y_train)
  dtest = xgb.DMatrix(X_test, label=y_test)
  
  results = []
```

Creating the grid search loop:

```{python}
for eta in etas:
    for depth in depths:
        print(f"Training model with eta={eta}, max_depth={depth}...")
        
        params = base_params.copy()
        params["eta"] = eta
        params["max_depth"] = depth
        
        watchlist = [(dtrain, "train"), (dtest, "test")]
        
        model = xgb.train(
            params=params,
            dtrain=dtrain,
            num_boost_round=num_rounds,
            evals=watchlist,
            early_stopping_rounds=early_stopping_rounds,
            verbose_eval=False  # suppress detailed output
        )
        
        best_rmse = model.best_score
        best_iter = model.best_iteration
        
        results.append({
            "eta": eta,
            "max_depth": depth,
            "best_rmse": best_rmse,
            "best_iteration": best_iter
        })
```

Convert the results to DataFrame

```{python}
results_df = pd.DataFrame(results)
print(results_df.sort_values("best_rmse"))
```

Lowest RMSE (\~0.8074) at eta=0.01, max_depth=10 indicates that a small learning rate and deeper trees best model the complex, noisy income data by capturing subtle interactions gradually without overfitting.

## Question 3: Which hyperparameter combination yielded the lowest validation RMSE, and what was that RMSE value?

Based on the results, do you think you need to expand the grid search other values or parameters? (Note: You do not need to actually search for more parameters, just discuss.)

**My Answer:** The optimal hyperparameters were eta = 0.01 and max_depth = 10, with a validation RMSE of \~0.807, indicating that slow learning and deeper trees best captured complex relationships without overfitting. Future tuning could explore smaller eta values (e.g., 0.005 or 0.001) and intermediate depths (8 or 12). Additional parameters like min_child_weight, subsample, or colsample_bytree may refine performance marginally, but current results are already strong.


### Task 3.4: Final Model Training
Using the best hyperparameters from Task 3.3, train a final XGBoost model on the full dataset.

```{python}


```

## Task 4: Neural Network Modeling (Keras)
Use the structure from `examples/neural_nets_simple.ipynb` to fit a fully connected neural network on the same encoded features and log-income target.
Use the same train/test split as in Task 2.1.

### Task 4.1: Prepare Data for Keras
If necessary, convert the training and test feature matrices and target vectors into NumPy arrays suitable for Keras.
__Note:__ You may have already done this in Task 2.1.

```{python}

```
